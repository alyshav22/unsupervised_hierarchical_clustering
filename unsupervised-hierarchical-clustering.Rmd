---
output:
  word_document: default
  pdf_document: default
  html_document: default
---
# Unsupervised Learning - Hierarchical Clustering 
### By Alysha Velasquez

> Hierarchical clustering provides an alternative approach to k-means clustering for distinguishing groups in the dataset. This approach can be subdivided into two types: agglomerative hierarchical clustering (AHC) and diverse hierarchical clustering. With AHC each observation is initially regarded as a cluster of its own. In contrast, diverse hierarchical clustering provides an inverse of AHC. It starts with the root, where all objects are included in one cluster. Afterward, the most heterogeneous clusters are iteratively divided until all observations are in their own cluster. The output of hierarchical clustering is a tree-based representation of the observations, called a dendrogram. The observations could be subdivided into groups by cutting the dendrogram at the desired similarity level. In this project, we will use the USArrests dataset to perform hierarchical clustering. The dataset contains the number of arrests for murder, assault, and rape for each of the 50 states in 1973. 

<br>


```{r}

# Loading and preparing data
data("USArrests")
my_data <- scale(USArrests)

# Compute dissimilarity matrix
d <- dist(my_data, method = "euclidean")

# Hierarchical clustering using Ward's method
res.hc <- hclust(d, method = "ward.D2" )

# Cut tree into 4 groups
grp <- cutree(res.hc, k = 4)

```

```{r}

# Visualize
plot(res.hc, cex = 0.6) # plot tree
rect.hclust(res.hc, k = 4, border = 2:5) # add rectangle

library("factoextra")
# Compute hierarchical clustering and cut into 4 clusters
res <- hcut(USArrests, k = 4, stand = TRUE)

# Visualize
fviz_dend(res, rect = TRUE, cex = 0.5,
          k_colors = c("#00AFBB","#2E9FDF", "#E7B800", "#FC4E07"))


```


